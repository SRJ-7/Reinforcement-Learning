\documentclass[12pt,a4paper]{article}

% --- Packages ---
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{courier}
\usepackage{multirow}

% --- Code style ---
\lstset{
    basicstyle=\footnotesize\ttfamily,
    backgroundcolor=\color{gray!10},
    frame=single,
    breaklines=true,
    captionpos=b,
    numbers=left,
    numberstyle=\tiny\color{gray},
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange}
}

\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    urlcolor=teal
}

% --- Title ---
\title{\textbf{Reinforcement Learning Assignment Report \\ Deep Q-Network and Policy Gradient Algorithms}}
\author{
    Soham Jiddewar \\
    Roll Number: ES22BTECH11017 \\
    Department of Electrical Engineering \\
    Indian Institute of Technology Hyderabad
}
\date{\today}

\begin{document}



\maketitle
\tableofcontents

\newpage



% ---------------------------------------------------

\section{Introduction}

This report presents the implementation and evaluation of two fundamental reinforcement learning algorithms:

\begin{itemize}
    \item \textbf{Deep Q-Network (DQN)}: A value-based method that approximates the optimal action-value function using deep neural networks
    \item \textbf{Policy Gradient (PG)}: A policy-based method that directly optimizes policy parameters through gradient ascent
\end{itemize}

\subsection{Environments}

The following Gymnasium environments were used:

\begin{itemize}
    \item \texttt{ALE/Pong-v5}: Atari Pong game (visual input)
    \item \texttt{MountainCar-v0}: Classic control problem with sparse rewards
    \item \texttt{CartPole-v1}: Pole balancing task
    \item \texttt{LunarLander-v3}: Spacecraft landing simulation
\end{itemize}

Evaluation is based on average episode return, convergence behavior, and stability across multiple runs.

\subsection{Implementation Framework}

\begin{itemize}
    \item \textbf{Framework}: PyTorch 2.5.1
    \item \textbf{Environment}: Gymnasium 1.2.1
    \item \textbf{Hardware}: NVIDIA GeForce RTX 3050 Laptop GPU
    \item \textbf{Command-line Interface}: Argparse for flexible hyperparameter configuration
\end{itemize}

% ---------------------------------------------------

\section{Problem 1: Deep Q-Network (DQN)}

\subsection{(a) Environment Setup and Random Agent}

The following code snippet demonstrates how to load the Gym environments and print the state and action spaces:

\begin{lstlisting}[language=Python, caption=Loading Gym Environments]

import gym

env = gym.make("MountainCar-v0")
print("State space:", env.observation_space)
print("Action space:", env.action_space)
\end{lstlisting}

\subsubsection{Environment Analysis}

\textbf{Pong-v5:}

\begin{itemize}
    \item \textbf{State Space}: RGB images (210×160×3 pixels)
    \item \textbf{Action Space}: Discrete(6) - \{NOOP, FIRE, UP, DOWN, UPFIRE, DOWNFIRE\}
    \item \textbf{Reward}: +1 for scoring, -1 for opponent scoring
    \item \textbf{Episode Termination}: First to 21 points
\end{itemize}

\textbf{MountainCar-v0:}

\begin{itemize}
    \item \textbf{State Space}: Box(2) - position $\in [-1.2, 0.6]$, velocity $\in [-0.07, 0.07]$
    \item \textbf{Action Space}: Discrete(3) - \{push left, no push, push right\}
    \item \textbf{Reward}: -1 per timestep until goal is reached
    \item \textbf{Episode Termination}: Goal reached (position $\geq 0.5$) or 200 steps
\end{itemize}

\begin{table}[H]
\centering
\caption{DQN Environment Specifications}
\begin{tabular}{lcccc}
\toprule
\textbf{Environment} & \textbf{State Dim} & \textbf{Actions} & \textbf{Challenge} & \textbf{Max Steps} \\
\midrule
Pong-v5 & (210,160,3) & 6 & Visual, Delayed Reward & Variable \\
MountainCar-v0 & 2 & 3 & Sparse Reward & 200 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{(b) DQN Implementation}

\subsubsection{Core Components}

The DQN algorithm employs three key innovations:

\begin{enumerate}
    \item \textbf{Experience Replay}: Stores transitions $(s, a, r, s', done)$ in a replay buffer ($D$) to break temporal correlations
    \item \textbf{Target Network}: Maintains a separate network $Q_{\theta^-}$ updated periodically
    \item \textbf{Double DQN}: Uses policy network for action selection and target network for value estimation
\end{enumerate}

The loss function is:
\[
L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma Q_{\theta^-}(s', \arg\max_{a'} Q_\theta(s', a')) - Q_\theta(s,a) \right)^2 \right]
\]

\subsubsection{Neural Network Architecture}

\textbf{Pong CNN Architecture:}
\begin{itemize}
    \item Input: 4 stacked 84×84 grayscale frames
    \item Conv1: 32 filters, 8×8 kernel, stride 4, ReLU
    \item Conv2: 64 filters, 4×4 kernel, stride 2, ReLU
    \item Conv3: 64 filters, 3×3 kernel, stride 1, ReLU
    \item FC1: 512 units, ReLU
    \item Output: 6 Q-values (one per action)
\end{itemize}

\textbf{MountainCar MLP Architecture:}
\begin{itemize}
    \item Input: 2-dimensional state
    \item Hidden1: 128 units, ReLU
    \item Hidden2: 128 units, ReLU
    \item Output: 3 Q-values
\end{itemize}

\subsubsection{Preprocessing for Pong}

\begin{enumerate}

    \item Convert RGB frames to grayscale.

\textbf{Pong CNN Architecture:}    \item Downsample the image.

\begin{itemize}    \item Subtract consecutive frames to capture motion.

    \item Input: 4 stacked 84×84 grayscale frames\end{enumerate}

    \item Conv1: 32 filters, 8×8 kernel, stride 4, ReLU

    \item Conv2: 64 filters, 4×4 kernel, stride 2, ReLU\subsection{(c) Results and Learning Curves}

    \item Conv3: 64 filters, 3×3 kernel, stride 1, ReLU\begin{figure}[H]

    \item FC1: 512 units, ReLU\centering

    \item Output: 6 Q-values (one per action)\includegraphics[width=0.8\textwidth]{dqn_mountaincar_curve.png}

\end{itemize}\caption{Learning curve for DQN on MountainCar-v0 (mean reward per episode).}

\end{figure}

\textbf{MountainCar MLP Architecture:}

\begin{itemize}\begin{figure}[H]

    \item Input: 2-dimensional state\centering

    \item Hidden1: 128 units, ReLU\includegraphics[width=0.8\textwidth]{dqn_pong_curve.png}

    \item Hidden2: 128 units, ReLU\caption{Learning curve for DQN on Pong-v0.}

    \item Output: 3 Q-values\end{figure}

\end{itemize}

\subsection{(d) Hyperparameter Study}

\subsubsection{Preprocessing for Pong}The learning rate was varied as follows: $\{1 \times 10^{-2}, 1 \times 10^{-3}, 1 \times 10^{-4}, 5 \times 10^{-5}\}$.

\begin{enumerate}

    \item Convert RGB to grayscale
    \item Crop to remove scoreboard (35:195 rows)
    \item Downsample to 84×84 pixels
    \item Normalize pixel values to [0, 1]
    \item Stack 4 consecutive frames for motion information
\end{enumerate}

\subsection{Hyperparameters}

\begin{table}[H]
\centering
\caption{DQN Hyperparameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{Pong} & \textbf{MountainCar} \\
\midrule
Learning Rate & $2.5 \times 10^{-4}$ & $1 \times 10^{-3}$ \\
Discount Factor ($\gamma$) & 0.99 & 0.99 \\
Replay Buffer Size & 50,000 & 50,000 \\
Batch Size & 32 & 32 \\
Target Update Frequency & 10,000 steps & 1,000 steps \\
Initial Epsilon ($\epsilon_0$) & 1.0 & 1.0 \\
Final Epsilon ($\epsilon_{min}$) & 0.02 & 0.01 \\
Epsilon Decay & 0.9999 & 0.995 \\
Optimizer & Adam & Adam \\
\bottomrule
\end{tabular}
\end{table}

\subsection{(c) Results and Learning Curves}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{dqn_mountaincar_curve.png}
\caption{Learning curve for DQN on MountainCar-v0 (mean reward per episode).}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{dqn_pong_curve.png}
\caption{Learning curve for DQN on Pong-v0.}
\end{figure}

\subsection{(d) Hyperparameter Study}

The learning rate was varied as follows: $\{1 \times 10^{-2}, 1 \times 10^{-3}, 1 \times 10^{-4}, 5 \times 10^{-5}\}$.

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{lr_comparison.png}
\caption{Effect of learning rate on DQN performance (MountainCar-v0).}
\end{figure}

\textbf{Observation:} Extremely high or low learning rates led to instability or slow convergence. The best trade-off was found at $\alpha = 1e{-3}$.

\subsection{(e) Discussion}

DQN successfully learned to solve the MountainCar problem, with smooth convergence after sufficient exploration. In Pong, preprocessing was crucial for learning meaningful motion-based representations. Training required several million steps for stable performance.

\subsection{Training Results on Pong-v5}

\textbf{Training Configuration:}

\begin{itemize}
    \item Total Training Steps: 3,000,000 (3M)
    \item Training divided into 60 batches of 50,000 steps each
    \item Total Episodes: 1,249
    \item Training Time: $\sim$6-7 hours on RTX 3050
\end{itemize}

\textbf{Performance Metrics:}

\begin{table}[H]
\centering
\caption{Pong DQN Final Performance}
\begin{tabular}{lc}
\toprule
\textbf{Metric} & \textbf{Value} \\
\midrule
Best Mean Reward (100 ep) & +11.75 \\
Last 100 Episodes Mean & +11.19 \\
Last 50 Episodes Mean & +10.76 \\
Win Rate (Last 100 ep) & 97\% \\
Max Reward Achieved & +20 \\
Final Epsilon & 0.02 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}

\begin{itemize}
    \item Agent achieved 97\% win rate against built-in opponent
    \item Average score of +11.19 indicates consistent winning performance
    \item In Pong, scores range from -21 to +21
    \item Agent successfully learned to track ball and position paddle
    \item Training converged after $\sim$2M steps, continued to improve through 3M
\end{itemize}

% ---------------------------------------------------

\section{Problem 2: Policy Gradient}

\subsection{(a) Environment Setup and Random Agent}

\begin{lstlisting}[language=Python, caption=Loading CartPole Environment]
env = gym.make("CartPole-v0")
print("State space:", env.observation_space)
print("Action space:", env.action_space)
\end{lstlisting}

\textbf{Observation:}

\begin{itemize}
    \item Continuous state space of dimension 4.
    \item Two discrete actions: left or right.
\end{itemize}

\subsection{(b) Implementation Details}

The policy gradient algorithm (REINFORCE) optimizes:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_t \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]
\]

Where $G_t$ is either:

\begin{itemize}
    \item \textbf{Total reward}: $G_0 = \sum_{t=0}^T \gamma^t r_{t+1}$
    \item \textbf{Reward-to-go}: $G_t = \sum_{t'=t}^T \gamma^{t'-t} r_{t'+1}$
\end{itemize}

To reduce variance, we use:
\[
A_t = G_t - b(s_t)
\]
where $b(s_t)$ is a baseline (e.g., value function). Advantage normalization ensures $\text{mean}(A_t) = 0$ and $\text{std}(A_t) = 1$.

\subsection{(c) Results and Comparisons}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{pg_cartpole_comparison.png}
\caption{Policy Gradient on CartPole-v0: with/without Reward-to-Go and Advantage Normalization.}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{pg_lunarlander_comparison.png}
\caption{Policy Gradient on LunarLander-v2: variance reduction effects.}
\end{figure}

\textbf{Observation:} Both reward-to-go and advantage normalization significantly stabilized training and accelerated convergence.

\subsection{(d) Effect of Batch Size}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{batch_size_effect.png}
\caption{Effect of batch size on Policy Gradient performance.}
\end{figure}

Larger batch sizes led to smoother gradients and more stable returns but increased computation per iteration.

\subsection{(e) Discussion}

Variance reduction techniques substantially improved learning stability, especially in environments with high reward stochasticity such as LunarLander.

% ---------------------------------------------------

\section{Problem 3: Extended Results}

\subsection{Pong DQN Training Results (3M steps)}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{pong_training_3M_results.png}
\caption{Pong DQN Training Results (3M steps): Learning curves showing episode rewards, reward distribution, loss progression, and performance across 60 training batches}
\end{figure}

\subsection{MountainCar DQN Results}

\textbf{Performance Metrics:}

\begin{itemize}
    \item Success Rate: 78-88\% (episodes where goal is reached)
    \item Average Reward: -147 to -154
    \item The agent learned to reach the goal but took many steps
    \item Successfully employed momentum-building strategy
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{mountaincar_training_results.png}
\caption{MountainCar DQN Training: Episode rewards and moving average showing learning progression}
\end{figure}

\subsection{Hyperparameter Study: Learning Rate}

A systematic study was conducted with four different learning rates:

\begin{itemize}
    \item $\alpha = 0.0001$
    \item $\alpha = 0.001$
    \item $\alpha = 0.01$
    \item $\alpha = 0.1$
\end{itemize}

\begin{table}[H]
\centering
\caption{Learning Rate Study Results (MountainCar)}
\begin{tabular}{lc}
\toprule
\textbf{Learning Rate} & \textbf{Best Mean Reward} \\
\midrule
$1 \times 10^{-4}$ & -199.45 \\
$1 \times 10^{-3}$ & -200.00 \\
$1 \times 10^{-2}$ & -200.00 \\
$1 \times 10^{-1}$ & -200.00 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{mountaincar_lr_comparison.png}
\caption{Learning Rate Comparison: Effect on MountainCar DQN performance}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item $\alpha = 0.0001$ showed slightly better performance
    \item Higher learning rates ($\geq 0.01$) led to instability
    \item Lower learning rates require more training time
    \item Optimal range: $[10^{-4}, 10^{-3}]$ for this environment
\end{itemize}

% ---------------------------------------------------
\section{Problem 3: Policy Gradient}

\subsection{Environment Analysis}

\subsubsection{CartPole-v1}
\begin{itemize}
    \item \textbf{State Space}: Box(4) - cart position, velocity, pole angle, pole angular velocity
    \item \textbf{Action Space}: Discrete(2) - push left or right
    \item \textbf{Reward}: +1 for each timestep pole remains upright
    \item \textbf{Solved Threshold}: Average reward $\geq$ 475 over 100 episodes
    \item \textbf{Episode Termination}: Pole angle $> \pm 12°$ or cart position $> \pm 2.4$ or 500 steps
\end{itemize}

\subsubsection{LunarLander-v3}
\begin{itemize}
    \item \textbf{State Space}: Box(8) - position, velocity, angle, angular velocity, leg contact
    \item \textbf{Action Space}: Discrete(4) - \{do nothing, fire left, fire main, fire right\}
    \item \textbf{Reward}: Based on distance to landing pad, velocity, crashes, fuel usage
    \item \textbf{Solved Threshold}: Average reward $\geq$ 200 over 100 episodes
    \item \textbf{Reward Range}: [-∞, +200+] (crash: -100, land: +100-140)
\end{itemize}

\subsection{Random Agent Baseline}

\textbf{CartPole-v1 Random Agent:}
\begin{itemize}
    \item Average Reward: $19.4 \pm 10.26$
    \item Max Reward: 42.0
    \item Min Reward: 8.0
\end{itemize}

\subsection{Policy Gradient Algorithm}

\subsubsection{REINFORCE with Variance Reduction}

The policy gradient theorem:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]
\]

\textbf{Return Computation:}
\begin{itemize}
    \item \textbf{Total Trajectory Reward}: $G_0 = \sum_{t=0}^T \gamma^t r_{t+1}$ (same for all timesteps)
    \item \textbf{Reward-to-Go}: $G_t = \sum_{t'=t}^T \gamma^{t'-t} r_{t'+1}$ (reduces variance)
\end{itemize}

\textbf{Baseline Functions:}
\begin{enumerate}
    \item \textbf{No Baseline}: $b = 0$
    \item \textbf{Constant}: $b = \mathbb{E}[G(\tau)] \approx \frac{1}{K} \sum_i G(\tau^i)$
    \item \textbf{Time-dependent}: $b_t = \frac{1}{K} \sum_i G_t(\tau^i)$
    \item \textbf{State-dependent}: $b(s) = V^\pi(s)$ (learned value function)
\end{enumerate}

\textbf{Advantage Estimation:}
\[
A_t = G_t - b(s_t)
\]

\textbf{Advantage Normalization:}
\[
\hat{A}_t = \frac{A_t - \mu(A)}{\sigma(A) + \epsilon}
\]
where $\mu(A)$ is mean, $\sigma(A)$ is standard deviation, and $\epsilon = 10^{-8}$ for numerical stability.

\subsection{Neural Network Architecture}

\textbf{Policy Network (Actor):}
\begin{itemize}
    \item Input: State dimension
    \item Hidden1: 128 units, ReLU
    \item Hidden2: 128 units, ReLU
    \item Output: Action probabilities (Softmax)
\end{itemize}

\textbf{Value Network (Critic - for state baseline):}
\begin{itemize}
    \item Input: State dimension
    \item Hidden1: 128 units, ReLU
    \item Hidden2: 128 units, ReLU
    \item Output: State value $V(s)$
\end{itemize}

\subsection{Hyperparameters}

\begin{table}[H]
\centering
\caption{Policy Gradient Hyperparameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{CartPole} & \textbf{LunarLander} \\
\midrule
Learning Rate (Policy) & 0.001 & 0.001 \\
Learning Rate (Value) & 0.001 & 0.001 \\
Discount Factor ($\gamma$) & 0.99 & 0.99 \\
Iterations & 500 & 500 \\
Batch Size (episodes) & 20 & 20 \\
Optimizer & Adam & Adam \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Results}

\subsubsection{CartPole-v1 Results}

\begin{table}[H]
\centering
\caption{CartPole-v1 Policy Gradient Comparison (500 iterations, batch size 20)}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{RTG} & \textbf{Norm} & \textbf{Baseline} & \textbf{Best Reward} \\
\midrule
Baseline & No & No & None & 39.75 \\
RTG Only & Yes & No & None & 24.25 \\
RTG + Norm & Yes & Yes & None & 362.55 \\
RTG + Norm + Constant & Yes & Yes & Constant & \textbf{404.85} \\
RTG + Norm + State & Yes & Yes & State V(s) & 64.40 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item RTG + Normalization + Constant Baseline achieved best performance: 404.85
    \item Close to solving threshold (475) but needed more iterations
    \item Normalization alone (without baseline) can hurt performance
    \item Baseline subtraction is critical for variance reduction
    \item 10.2x improvement from baseline (39.75 → 404.85)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{CartPole-v1_pg_comparison.png}
\caption{CartPole-v1 Policy Gradient: Learning curves comparing different variance reduction techniques}
\end{figure}

\subsubsection{LunarLander-v3 Results}

\begin{table}[H]
\centering
\caption{LunarLander-v3 Policy Gradient Comparison (500 iterations, batch size 20)}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{RTG} & \textbf{Norm} & \textbf{Baseline} & \textbf{Best Reward} \\
\midrule
Baseline & No & No & None & -113.09 \\
RTG Only & Yes & No & None & 133.46 \\
RTG + Norm & Yes & Yes & None & 187.90 \\
RTG + Norm + Constant & Yes & Yes & Constant & \textbf{211.65} \\
RTG + Norm + State & Yes & Yes & State V(s) & 191.19 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item RTG + Norm + Constant Baseline performed best: 211.65
    \item Successfully solved the environment (threshold: 200)
    \item Dramatic improvement from -113.09 to +211.65 (324.74 point increase)
    \item All variance reduction techniques showed significant gains
    \item Reward-to-go is essential for environments with delayed rewards
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{LunarLander-v3_pg_comparison.png}
\caption{LunarLander-v3 Policy Gradient: Learning curves showing dramatic improvement with variance reduction}
\end{figure}

\subsection{Command-Line Interface}

The implementation includes a flexible command-line interface supporting all configurations:

\begin{lstlisting}[language=bash, caption=Policy Gradient Command-Line Usage]
# With all variance reduction techniques
python policy_gradient.py --env CartPole-v1 --reward_to_go \
    --normalize_advantages --baseline state --iterations 500 \
    --batch_size 20

# Run full comparison automatically
python policy_gradient.py --env CartPole-v1 --iterations 500 \
    --batch_size 20 --compare

# LunarLander training
python policy_gradient.py --env LunarLander-v3 --reward_to_go \
    --normalize_advantages --baseline constant --iterations 500 \
    --batch_size 20 --compare
\end{lstlisting}

% ---------------------------------------------------
\section{Analysis and Discussion}

\subsection{DQN vs Policy Gradient}

\begin{table}[H]
\centering
\caption{Comparison of DQN and Policy Gradient Methods}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{DQN} & \textbf{Policy Gradient} \\
\midrule
Learning Type & Value-based & Policy-based \\
Action Space & Discrete only & Discrete \& Continuous \\
Sample Efficiency & High (replay buffer) & Lower (on-policy) \\
Stability & Requires target network & Requires variance reduction \\
Convergence & Can oscillate & Smoother with baselines \\
Memory Usage & High (replay buffer) & Low \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Variance Reduction Analysis}

\textbf{Impact of Each Technique:}

\begin{enumerate}
    \item \textbf{Reward-to-Go}:
    \begin{itemize}
        \item Reduces variance by using causal returns
        \item Essential for environments with delayed rewards
        \item CartPole: Minimal improvement alone
        \item LunarLander: 246.55 point improvement (-113.09 → 133.46)
    \end{itemize}
    
    \item \textbf{Advantage Normalization}:
    \begin{itemize}
        \item Standardizes advantage estimates
        \item Can hurt performance without good baseline
        \item Works best when combined with baseline subtraction
        \item Ensures stable gradient magnitudes
    \end{itemize}
    
    \item \textbf{Baseline Subtraction}:
    \begin{itemize}
        \item Most critical variance reduction technique
        \item Constant baseline: Simple, effective
        \item State-dependent baseline: Best theoretical properties
        \item CartPole: 365.10 point improvement (39.75 → 404.85)
    \end{itemize}
\end{enumerate}

\subsection{Computational Requirements}

\begin{table}[H]
\centering
\caption{Training Time and Computational Requirements}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Steps/Iterations} & \textbf{Time} & \textbf{Episodes} \\
\midrule
Pong DQN (3M) & 3,000,000 & $\sim$6-7 hours & 1,249 \\
MountainCar DQN & 100,000 & $\sim$30 min & $\sim$500 \\
CartPole PG (5 configs) & 500 × 5 & $\sim$20 min & $\sim$50,000 \\
LunarLander PG (5 configs) & 500 × 5 & $\sim$2 hours & $\sim$50,000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Challenges and Solutions}

\textbf{Challenges Encountered:}
\begin{enumerate}
    \item \textbf{Pong Training Stability}:
    \begin{itemize}
        \item Problem: GPU memory overflow with large replay buffer
        \item Solution: Batch training (50K step chunks), memory cleanup between batches
    \end{itemize}
    
    \item \textbf{MountainCar Sparse Rewards}:
    \begin{itemize}
        \item Problem: Agent struggles to discover goal initially
        \item Solution: Long exploration period, careful epsilon decay
    \end{itemize}
    
    \item \textbf{CartPole High Variance}:
    \begin{itemize}
        \item Problem: Policy gradient unstable without variance reduction
        \item Solution: Implemented multiple baseline types and normalization
    \end{itemize}
    
    \item \textbf{LunarLander Box2D Dependency}:
    \begin{itemize}
        \item Problem: Box2D physics library installation issues on Windows
        \item Solution: Used `pip install Box2D --no-build-isolation`
    \end{itemize}
\end{enumerate}

% ---------------------------------------------------
\section{Conclusions}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{DQN Performance}:
    \begin{itemize}
        \item Successfully mastered Pong with 97\% win rate after 3M steps
        \item Achieved 78-88\% success rate on MountainCar
        \item CNN architecture essential for visual environments
        \item Experience replay and target networks crucial for stability
    \end{itemize}
    
    \item \textbf{Policy Gradient Performance}:
    \begin{itemize}
        \item Variance reduction techniques critical for success
        \item RTG + Norm + Baseline achieved 10x improvement on CartPole
        \item Successfully solved LunarLander (211.65 vs 200 threshold)
        \item Baseline subtraction most impactful single technique
    \end{itemize}
    
    \item \textbf{Hyperparameter Sensitivity}:
    \begin{itemize}
        \item Learning rate optimal range: $[10^{-4}, 10^{-3}]$
        \item Batch size affects stability vs computation trade-off
        \item Epsilon decay rate influences exploration-exploitation balance
    \end{itemize}
\end{enumerate}

\subsection{Comparative Analysis}

\textbf{When to Use DQN:}
\begin{itemize}
    \item Discrete action spaces
    \item Visual/high-dimensional observations
    \item When sample efficiency is important
    \item Environments where off-policy learning is beneficial
\end{itemize}

\textbf{When to Use Policy Gradient:}
\begin{itemize}
    \item Continuous action spaces
    \item When direct policy optimization is needed
    \item Simpler implementation for discrete spaces
    \item When on-policy learning is acceptable
\end{itemize}

\subsection{Future Improvements}

\begin{enumerate}
    \item \textbf{DQN Extensions}:
    \begin{itemize}
        \item Implement Dueling DQN architecture
        \item Try Prioritized Experience Replay
        \item Explore Rainbow DQN combining multiple improvements
    \end{itemize}
    
    \item \textbf{Policy Gradient Extensions}:
    \begin{itemize}
        \item Implement Actor-Critic methods (A2C/A3C)
        \item Explore Proximal Policy Optimization (PPO)
        \item Test Generalized Advantage Estimation (GAE)
    \end{itemize}
    
    \item \textbf{Training Optimization}:
    \begin{itemize}
        \item Implement distributed training
        \item Automatic hyperparameter tuning
        \item Early stopping based on performance metrics
    \end{itemize}
\end{enumerate}

% ---------------------------------------------------
\section{Code Structure and Reproducibility}

\subsection{File Organization}
\begin{lstlisting}
assignment/
├── dqn_pong.py                    # Pong DQN implementation
├── dqn_pong_continue.py           # Continue training from checkpoint
├── dqn_quickstart.py              # MountainCar DQN
├── mountaincar_hyperparameter_study.py  # LR study
├── evaluate_pong_3M.py            # Pong evaluation with rendering
├── evaluate_mountaincar.py        # MountainCar evaluation
├── policy_gradient.py             # PG with variance reduction
├── evaluate_best_pg.py            # PG model evaluation
└── 
    ├── pong_dqn_3M_final.pth      # Final Pong model
    ├── mountaincar_dqn_final.pth  # MountainCar model
    ├── CartPole-v1_*_best.pth     # CartPole PG models (5)
    ├── LunarLander-v3_*_best.pth  # LunarLander PG models (5)
    └── *.png                      # Training plots
\end{lstlisting}

\subsection{Reproducibility}

All experiments can be reproduced using the provided code and hyperparameters. Random seeds were set (SEED=42) for consistency. The command-line interface allows exact replication of any configuration.

% ---------------------------------------------------
\section{References}

\begin{enumerate}
    \item Mnih, V., et al. (2015). \textit{Human-level control through deep reinforcement learning}. Nature, 518(7540), 529-533.
    
    \item Van Hasselt, H., Guez, A., \& Silver, D. (2016). \textit{Deep reinforcement learning with double Q-learning}. In AAAI Conference on Artificial Intelligence.
    
    \item Williams, R. J. (1992). \textit{Simple statistical gradient-following algorithms for connectionist reinforcement learning}. Machine Learning, 8(3-4), 229-256.
    
    \item Sutton, R. S., \& Barto, A. G. (2018). \textit{Reinforcement Learning: An Introduction} (2nd ed.). MIT Press.
    
    \item Schulman, J., et al. (2015). \textit{High-dimensional continuous control using generalized advantage estimation}. arXiv preprint arXiv:1506.02438.
    
    \item Gymnasium Documentation. \url{https://gymnasium.farama.org/}
    
    \item PyTorch Documentation. \url{https://pytorch.org/docs/}
\end{enumerate}

\end{document}

