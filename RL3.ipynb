{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971d04e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install gym==0.21.0\n",
    "!{sys.executable} -m pip install numpy==1.19.5\n",
    "!{sys.executable} -m pip install \"gym[atari]\"==0.21.0\n",
    "!{sys.executable} -m pip install atari-py==0.2.9\n",
    "!{sys.executable} -m pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1015c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14c2068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Collecting gym==0.21.0\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n",
      "Collecting gym==0.21.0\n",
      "  Using cached gym-0.21.0.tar.gz (1.5 MB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [3 lines of output]\n",
      "      D:\\codeforce\\myenv3.11.8\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:289: UserWarning: Unknown distribution option: 'tests_require'\n",
      "        warnings.warn(msg)\n",
      "      error in gym setup command: 'extras_require' must be a dictionary whose values are strings or lists of strings containing valid project/version requirement specifiers.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall gymnasium\n",
    "!pip install gym==0.21.0\n",
    "!pip install numpy==1.23.5\n",
    "!pip install gym[atari]\n",
    "!pip install atari-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417c4b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install autorom[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6088e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "GYM ENVIRONMENT EXPLORATION FOR DQN\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EXPLORING ENVIRONMENT: MountainCar-v0\n",
      "======================================================================\n",
      "\n",
      "STATE SPACE:\n",
      "  Type: <class 'gymnasium.spaces.box.Box'>\n",
      "  Shape: (2,)\n",
      "  Low bounds: [-1.2  -0.07]\n",
      "  High bounds: [0.6  0.07]\n",
      "\n",
      "ACTION SPACE:\n",
      "  Type: <class 'gymnasium.spaces.discrete.Discrete'>\n",
      "  Number of actions: 3\n",
      "\n",
      "======================================================================\n",
      "RANDOM AGENT ANALYSIS\n",
      "======================================================================\n",
      "\n",
      "Episode 1:\n",
      "  Total Reward: -200.00\n",
      "  Episode Length: 200 steps\n",
      "  Average Reward per Step: -1.0000\n",
      "Episode 2:\n",
      "  Total Reward: -200.00\n",
      "  Episode Length: 200 steps\n",
      "  Average Reward per Step: -1.0000\n",
      "Episode 3:\n",
      "  Total Reward: -200.00\n",
      "  Episode Length: 200 steps\n",
      "  Average Reward per Step: -1.0000\n",
      "Episode 4:\n",
      "  Total Reward: -200.00\n",
      "  Episode Length: 200 steps\n",
      "  Average Reward per Step: -1.0000\n",
      "Episode 5:\n",
      "  Total Reward: -200.00\n",
      "  Episode Length: 200 steps\n",
      "  Average Reward per Step: -1.0000\n",
      "\n",
      "======================================================================\n",
      "SUMMARY STATISTICS\n",
      "======================================================================\n",
      "\n",
      "Average Episode Reward: -200.00 ± 0.00\n",
      "Average Episode Length: 200.0 ± 0.0\n",
      "Min/Max Episode Reward: -200.00 / -200.00\n",
      "Min/Max Episode Length: 200 / 200\n",
      "\n",
      "Unique Reward Values: [-1.]\n",
      "Reward Distribution:\n",
      "  Reward   -1.0:  1000 times (100.0%)\n",
      "\n",
      "Sample States (first 3):\n",
      "  State 0: [-0.40707904  0.        ]\n",
      "  State 1: [-4.0693524e-01  1.4379078e-04]\n",
      "  State 2: [-0.40764868 -0.00071343]\n",
      "\n",
      "======================================================================\n",
      "Note: Pong-v0 episodes can be very long. Running with reduced episodes.\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "EXPLORING ENVIRONMENT: Pong-v0\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "ename": "NameNotFound",
     "evalue": "Environment `Pong` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameNotFound\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 179\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNote: Pong-v0 episodes can be very long. Running with reduced episodes.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    178\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m*\u001b[32m70\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m179\u001b[39m pong_results = \u001b[43mexplore_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mPong-v0\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_steps\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m \u001b[38;5;66;03m# Plot comparison\u001b[39;00m\n\u001b[32m    182\u001b[39m plot_results(mountaincar_results, pong_results)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mexplore_environment\u001b[39m\u001b[34m(env_name, num_episodes, max_steps)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m70\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Load environment\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m env = \u001b[43mgym\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# Print State Space Information\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mSTATE SPACE:\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codeforce\\myenv3.11.8\\Lib\\site-packages\\gymnasium\\envs\\registration.py:681\u001b[39m, in \u001b[36mmake\u001b[39m\u001b[34m(id, max_episode_steps, disable_env_checker, **kwargs)\u001b[39m\n\u001b[32m    678\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;28mstr\u001b[39m)\n\u001b[32m    680\u001b[39m     \u001b[38;5;66;03m# The environment name can include an unloaded module in \"module:env_name\" style\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m681\u001b[39m     env_spec = \u001b[43m_find_spec\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(env_spec, EnvSpec)\n\u001b[32m    685\u001b[39m \u001b[38;5;66;03m# Update the env spec kwargs with the `make` kwargs\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codeforce\\myenv3.11.8\\Lib\\site-packages\\gymnasium\\envs\\registration.py:526\u001b[39m, in \u001b[36m_find_spec\u001b[39m\u001b[34m(env_id)\u001b[39m\n\u001b[32m    520\u001b[39m     logger.warn(\n\u001b[32m    521\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing the latest versioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnew_env_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    522\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33minstead of the unversioned environment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    523\u001b[39m     )\n\u001b[32m    525\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m env_spec \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m526\u001b[39m     \u001b[43m_check_version_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    527\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error.Error(\n\u001b[32m    528\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo registered env with id: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. Did you register it, or import the package that registers it? Use `gymnasium.pprint_registry()` to see all of the registered environments.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    529\u001b[39m     )\n\u001b[32m    531\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m env_spec\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codeforce\\myenv3.11.8\\Lib\\site-packages\\gymnasium\\envs\\registration.py:392\u001b[39m, in \u001b[36m_check_version_exists\u001b[39m\u001b[34m(ns, name, version)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m get_env_id(ns, name, version) \u001b[38;5;129;01min\u001b[39;00m registry:\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m392\u001b[39m \u001b[43m_check_name_exists\u001b[49m\u001b[43m(\u001b[49m\u001b[43mns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    393\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    394\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\codeforce\\myenv3.11.8\\Lib\\site-packages\\gymnasium\\envs\\registration.py:369\u001b[39m, in \u001b[36m_check_name_exists\u001b[39m\u001b[34m(ns, name)\u001b[39m\n\u001b[32m    366\u001b[39m namespace_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m in namespace \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mns\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ns \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    367\u001b[39m suggestion_msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m Did you mean: `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m`?\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m suggestion \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m error.NameNotFound(\n\u001b[32m    370\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEnvironment `\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m` doesn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt exist\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnamespace_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msuggestion_msg\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    371\u001b[39m )\n",
      "\u001b[31mNameNotFound\u001b[39m: Environment `Pong` doesn't exist."
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "\n",
    "def explore_environment(env_name, num_episodes=5, max_steps=1000):\n",
    "    \"\"\"\n",
    "    Load environment, print state/action spaces, and run random agent.\n",
    "    \n",
    "    Args:\n",
    "        env_name: Name of the Gym environment\n",
    "        num_episodes: Number of episodes to run with random agent\n",
    "        max_steps: Maximum steps per episode\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"EXPLORING ENVIRONMENT: {env_name}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    # Load environment\n",
    "    env = gym.make(env_name)\n",
    "    \n",
    "    # Print State Space Information\n",
    "    print(\"STATE SPACE:\")\n",
    "    print(f\"  Type: {type(env.observation_space)}\")\n",
    "    print(f\"  Shape: {env.observation_space.shape}\")\n",
    "    \n",
    "    if hasattr(env.observation_space, 'low') and hasattr(env.observation_space, 'high'):\n",
    "        print(f\"  Low bounds: {env.observation_space.low}\")\n",
    "        print(f\"  High bounds: {env.observation_space.high}\")\n",
    "    \n",
    "    # Print Action Space Information\n",
    "    print(\"\\nACTION SPACE:\")\n",
    "    print(f\"  Type: {type(env.action_space)}\")\n",
    "    if isinstance(env.action_space, gym.spaces.Discrete):\n",
    "        print(f\"  Number of actions: {env.action_space.n}\")\n",
    "    else:\n",
    "        print(f\"  Shape: {env.action_space.shape}\")\n",
    "    \n",
    "    # Run random agent\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"RANDOM AGENT ANALYSIS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "    state_samples = []\n",
    "    reward_history = []\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        steps = 0\n",
    "        episode_reward_list = []\n",
    "        \n",
    "        # Store initial state\n",
    "        if episode == 0:\n",
    "            state_samples.append(state.copy())\n",
    "        \n",
    "        for step in range(max_steps):\n",
    "            # Random action\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "            # Step environment (old Gym API returns 4 values)\n",
    "            next_state, reward, done, info = env.step(action)\n",
    "            \n",
    "            episode_reward += reward\n",
    "            episode_reward_list.append(reward)\n",
    "            steps += 1\n",
    "            \n",
    "            # Store sample states\n",
    "            if episode == 0 and step < 5:\n",
    "                state_samples.append(next_state.copy())\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        episode_lengths.append(steps)\n",
    "        reward_history.extend(episode_reward_list)\n",
    "        \n",
    "        print(f\"Episode {episode + 1}:\")\n",
    "        print(f\"  Total Reward: {episode_reward:.2f}\")\n",
    "        print(f\"  Episode Length: {steps} steps\")\n",
    "        print(f\"  Average Reward per Step: {episode_reward/steps:.4f}\")\n",
    "    \n",
    "    # Summary Statistics\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"SUMMARY STATISTICS\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(f\"Average Episode Reward: {np.mean(episode_rewards):.2f} ± {np.std(episode_rewards):.2f}\")\n",
    "    print(f\"Average Episode Length: {np.mean(episode_lengths):.1f} ± {np.std(episode_lengths):.1f}\")\n",
    "    print(f\"Min/Max Episode Reward: {np.min(episode_rewards):.2f} / {np.max(episode_rewards):.2f}\")\n",
    "    print(f\"Min/Max Episode Length: {np.min(episode_lengths)} / {np.max(episode_lengths)}\")\n",
    "    \n",
    "    # Reward distribution\n",
    "    unique_rewards = np.unique(reward_history)\n",
    "    print(f\"\\nUnique Reward Values: {unique_rewards}\")\n",
    "    print(\"Reward Distribution:\")\n",
    "    for reward_val in unique_rewards:\n",
    "        count = reward_history.count(reward_val)\n",
    "        percentage = (count / len(reward_history)) * 100\n",
    "        print(f\"  Reward {reward_val:6.1f}: {count:5d} times ({percentage:5.1f}%)\")\n",
    "    \n",
    "    # Sample states\n",
    "    print(\"\\nSample States (first 3):\")\n",
    "    for i, state in enumerate(state_samples[:3]):\n",
    "        if len(state) <= 10:  # For small state spaces\n",
    "            print(f\"  State {i}: {state}\")\n",
    "        else:  # For large state spaces (like images)\n",
    "            print(f\"  State {i}: Shape {state.shape}, \"\n",
    "                  f\"Range [{state.min():.2f}, {state.max():.2f}], \"\n",
    "                  f\"Mean {state.mean():.2f}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    return {\n",
    "        'episode_rewards': episode_rewards,\n",
    "        'episode_lengths': episode_lengths,\n",
    "        'reward_history': reward_history\n",
    "    }\n",
    "\n",
    "def plot_results(mountaincar_results, pong_results):\n",
    "    \"\"\"Plot comparison of the two environments.\"\"\"\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    \n",
    "    # MountainCar episode rewards\n",
    "    axes[0, 0].plot(mountaincar_results['episode_rewards'], marker='o')\n",
    "    axes[0, 0].set_xlabel('Episode')\n",
    "    axes[0, 0].set_ylabel('Total Reward')\n",
    "    axes[0, 0].set_title('MountainCar-v0: Episode Rewards')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # MountainCar episode lengths\n",
    "    axes[0, 1].plot(mountaincar_results['episode_lengths'], marker='o', color='orange')\n",
    "    axes[0, 1].set_xlabel('Episode')\n",
    "    axes[0, 1].set_ylabel('Episode Length')\n",
    "    axes[0, 1].set_title('MountainCar-v0: Episode Lengths')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pong episode rewards\n",
    "    axes[1, 0].plot(pong_results['episode_rewards'], marker='o', color='green')\n",
    "    axes[1, 0].set_xlabel('Episode')\n",
    "    axes[1, 0].set_ylabel('Total Reward')\n",
    "    axes[1, 0].set_title('Pong-v0: Episode Rewards')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Pong episode lengths\n",
    "    axes[1, 1].plot(pong_results['episode_lengths'], marker='o', color='red')\n",
    "    axes[1, 1].set_xlabel('Episode')\n",
    "    axes[1, 1].set_ylabel('Episode Length')\n",
    "    axes[1, 1].set_title('Pong-v0: Episode Lengths')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('environment_comparison.png', dpi=150, bbox_inches='tight')\n",
    "    print(\"\\nPlot saved as 'environment_comparison.png'\")\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"GYM ENVIRONMENT EXPLORATION FOR DQN\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Explore MountainCar-v0\n",
    "    mountaincar_results = explore_environment('MountainCar-v0', num_episodes=5, max_steps=200)\n",
    "    \n",
    "    # Explore Pong-v0 (fewer episodes due to longer runtime)\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"Note: Pong-v0 episodes can be very long. Running with reduced episodes.\")\n",
    "    print(\"=\"*70)\n",
    "    pong_results = explore_environment('Pong-v0', num_episodes=3, max_steps=2000)\n",
    "\n",
    "    plot_results(mountaincar_results, pong_results)\n",
    "    \n",
    "    # Key Observations\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"KEY OBSERVATIONS\")\n",
    "    print(\"=\"*70)\n",
    "    print(\"\"\"\n",
    "MountainCar-v0:\n",
    "- State Space: 2D continuous (position, velocity)\n",
    "  * Position: [-1.2, 0.6] (negative = left, positive = right)\n",
    "  * Velocity: [-0.07, 0.07] (negative = leftward, positive = rightward)\n",
    "- Action Space: Discrete with 3 actions\n",
    "  * Action 0: Push Left\n",
    "  * Action 1: No Push (coast)\n",
    "  * Action 2: Push Right\n",
    "- Reward Structure: -1 for each time step until goal is reached at position 0.5\n",
    "- Challenge: Sparse rewards make learning difficult; agent must learn\n",
    "  to build momentum by going back and forth\n",
    "- Episode Length: Maximum 200 steps\n",
    "- Random Agent Performance: Always gets reward of -200 (never reaches goal)\n",
    "- Key Learning Challenge: Delayed reward problem - actions early in episode\n",
    "  affect ability to reach goal later\n",
    "\n",
    "Pong-v0:\n",
    "- State Space: High-dimensional (210x160x3 RGB image pixels = 100,800 values!)\n",
    "  * Each pixel has 3 color channels (Red, Green, Blue)\n",
    "  * Values range from 0-255\n",
    "- Action Space: Discrete with 6 actions\n",
    "  * Action 0: NOOP (no operation)\n",
    "  * Action 1: FIRE (not useful in Pong)\n",
    "  * Action 2: Move paddle UP\n",
    "  * Action 3: Move paddle DOWN\n",
    "  * Actions 4, 5: RIGHT-FIRE, LEFT-FIRE (not useful in Pong)\n",
    "  * Effectively only actions 0, 2, 3 are needed\n",
    "- Reward Structure: \n",
    "  * +1 for winning a volley (opponent misses ball)\n",
    "  * -1 for losing a volley (agent misses ball)\n",
    "  * 0 for most time steps (when ball is in play)\n",
    "- Challenge: High-dimensional visual input requires preprocessing\n",
    "- Episode Length: Varies (game ends when one player reaches 21 points)\n",
    "- Random Agent Performance: Typically loses badly (around -21 total reward)\n",
    "\n",
    "Recommended Preprocessing for Pong (for DQN):\n",
    "1. Convert RGB to grayscale (reduces from 3 channels to 1)\n",
    "2. Downsample image (e.g., 84x84 instead of 210x160)\n",
    "3. Stack 4 consecutive frames to capture motion/velocity\n",
    "4. Frame skipping (e.g., repeat action for 4 frames) to reduce computation\n",
    "5. Normalize pixel values to [0, 1] range\n",
    "\n",
    "These preprocessing steps reduce state space from 100,800 to ~28,224 values\n",
    "(84x84x4) and help the network learn motion patterns.\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e8eac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv3.11.8 (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
