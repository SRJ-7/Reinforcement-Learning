\documentclass[12pt,a4paper]{article}\documentclass[12pt,a4paper]{article}



% --- Packages ---% --- Packages ---

\usepackage[margin=1in]{geometry}\usepackage[margin=1in]{geometry}

\usepackage{amsmath, amssymb}\usepackage{amsmath, amssymb}

\usepackage{graphicx}\usepackage{graphicx}

\usepackage{float}\usepackage{float}

\usepackage{caption}\usepackage{caption}

\usepackage{subcaption}\usepackage{subcaption}

\usepackage{booktabs}\usepackage{booktabs}

\usepackage{hyperref}\usepackage{hyperref}

\usepackage{xcolor}\usepackage{xcolor}

\usepackage{listings}\usepackage{listings}

\usepackage{courier}\usepackage{courier}

\usepackage{multirow}

% --- Code style ---

% --- Code style ---\lstset{

\lstset{

    basicstyle=\footnotesize\ttfamily,    basicstyle=\footnotesize\ttfamily,

    backgroundcolor=\color{gray!10},    backgroundcolor=\color{gray!10},

    frame=single,    frame=single,

    breaklines=true,    breaklines=true,

    captionpos=b,    captionpos=b,

    numbers=left,    numbers=left,

    numberstyle=\tiny\color{gray},    numberstyle=\tiny\color{gray},

    keywordstyle=\color{blue},    keywordstyle=\color{blue},

    commentstyle=\color{gray},    commentstyle=\color{gray},

    stringstyle=\color{orange}    stringstyle=\color{orange}

}

}

\hypersetup{\hypersetup{

    colorlinks=true,

    linkcolor=blue,    colorlinks=true,

    urlcolor=teal    linkcolor=blue,

}    urlcolor=teal



% --- Title ---}

\title{\textbf{Reinforcement Learning Assignment Report\\Deep Q-Network and Policy Gradient Algorithms}}

\author{% --- Title ---

    Soham Jiddewar \\\title{\textbf{Assignment 3 Report: Deep Q-Network and Policy Gradient Algorithms}}

    Roll Number: ES22BTECH11017 \\\author{

    Department of Electrical Engineering \\    Soham Jiddewar \\

    Indian Institute of Technology Hyderabad    Roll Number: ES22BTECH11017 \\

}}

\date{\today}\date{}



\begin{document}\begin{document}



\maketitle
\tableofcontents

\newpage\newpage



% ---------------------------------------------------% ---------------------------------------------------

\section{Introduction}\section{Introduction}

This report presents the implementation and evaluation of two fundamental reinforcement learning algorithms:This assignment focuses on the implementation and evaluation of two key reinforcement learning algorithms:

\begin{itemize}\begin{itemize}

    \item \textbf{Deep Q-Network (DQN)}: A value-based method that approximates the optimal action-value function using deep neural networks    \item \textbf{Deep Q-Network (DQN)}: a value-based method that approximates the optimal Q-function using deep neural networks.

    \item \textbf{Policy Gradient (PG)}: A policy-based method that directly optimizes policy parameters through gradient ascent    \item \textbf{Policy Gradient (PG)}: a policy-based method that directly optimizes the parameters of the policy using gradient ascent.

\end{itemize}\end{itemize}



\subsection{Environments}The environments used for experimentation include:

The following Gymnasium environments were used:\begin{itemize}

\begin{itemize}    \item \texttt{MountainCar-v0}

    \item \texttt{ALE/Pong-v5}: Atari Pong game (visual input)    \item \texttt{Pong-v0}

    \item \texttt{MountainCar-v0}: Classic control problem with sparse rewards    \item \texttt{CartPole-v0}

    \item \texttt{CartPole-v1}: Pole balancing task    \item \texttt{LunarLander-v2}

    \item \texttt{LunarLander-v3}: Spacecraft landing simulation\end{itemize}

\end{itemize}

Evaluation is based on average episode return, convergence behavior, and stability across multiple runs.

\subsection{Implementation Framework}

\begin{itemize}% ---------------------------------------------------

    \item \textbf{Framework}: PyTorch 2.5.1\section{Problem 1: Deep Q-Learning}

    \item \textbf{Environment}: Gymnasium 1.2.1

    \item \textbf{Hardware}: NVIDIA GeForce RTX 3050 Laptop GPU\subsection{(a) Environment Setup and Random Agent}

    \item \textbf{Command-line Interface}: Argparse for flexible hyperparameter configurationThe following code snippet demonstrates how to load the Gym environments and print the state and action spaces:

\end{itemize}\begin{lstlisting}[language=Python, caption=Loading Gym Environments]

import gym

% ---------------------------------------------------

\section{Problem 1: Deep Q-Network (DQN)}env = gym.make("MountainCar-v0")

print("State space:", env.observation_space)

\subsection{Environment Analysis}print("Action space:", env.action_space)

\end{lstlisting}

\subsubsection{Pong-v5}

\begin{itemize}\textbf{Observations:}

    \item \textbf{State Space}: RGB images (210×160×3 pixels)\begin{itemize}

    \item \textbf{Action Space}: Discrete(6) - \{NOOP, FIRE, RIGHT, LEFT, RIGHTFIRE, LEFTFIRE\}    \item The state consists of position and velocity of the car.

    \item \textbf{Reward}: +1 for scoring, -1 for opponent scoring    \item There are three discrete actions: push left, no push, and push right.

    \item \textbf{Episode Termination}: First to 21 points    \item Each step where the goal is not reached gives a reward of -1.

\end{itemize}\end{itemize}



\subsubsection{MountainCar-v0}\begin{table}[H]

\begin{itemize}\centering

    \item \textbf{State Space}: Box(2) - position $\in [-1.2, 0.6]$, velocity $\in [-0.07, 0.07]$\caption{Environment Summary}

    \item \textbf{Action Space}: Discrete(3) - \{push left, no push, push right\}\begin{tabular}{lcccc}

    \item \textbf{Reward}: -1 per timestep until goal is reached\toprule

    \item \textbf{Episode Termination}: Goal reached (position $\geq 0.5$) or 200 steps\textbf{Environment} & \textbf{State Dim} & \textbf{Action Space} & \textbf{Reward Range} & \textbf{Episode Limit} \\

\end{itemize}\midrule

MountainCar-v0 & 2 & 3 & -1 per step & 200 \\

\begin{table}[H]Pong-v0 & (210,160,3) & 6 & +1 / -1 & 21 points \\

\centering\bottomrule

\caption{DQN Environment Specifications}\end{tabular}

\begin{tabular}{lcccc}\end{table}

\toprule

\textbf{Environment} & \textbf{State Dim} & \textbf{Actions} & \textbf{Challenge} & \textbf{Max Steps} \\\subsection{(b) DQN Implementation}

\midruleThe DQN algorithm uses:

Pong-v5 & (210,160,3) & 6 & Visual, Delayed Reward & Variable \\\begin{itemize}

MountainCar-v0 & 2 & 3 & Sparse Reward & 200 \\    \item A Q-network $Q_\theta(s,a)$ parameterized by weights $\theta$.

\bottomrule    \item A target network $Q_{\theta^-}$ updated every fixed interval.

\end{tabular}    \item Experience replay buffer to break correlation between samples.

\end{table}\end{itemize}



\subsection{DQN Algorithm Implementation}The loss is given by:

\[

\subsubsection{Core Components}L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma \max_{a'} Q_{\theta^-}(s',a') - Q_\theta(s,a) \right)^2 \right]

The DQN algorithm employs three key innovations:\]



\begin{enumerate}\textbf{Key Hyperparameters:}

    \item \textbf{Experience Replay}: Stores transitions $(s, a, r, s', done)$ in a replay buffer ($D$) to break temporal correlations\begin{itemize}

    \item \textbf{Target Network}: Maintains a separate network $Q_{\theta^-}$ updated periodically    \item Learning rate: 1e-3

    \item \textbf{Double DQN}: Uses policy network for action selection and target network for value estimation    \item Discount factor ($\gamma$): 0.99

\end{enumerate}    \item Replay buffer size: 50,000

    \item Mini-batch size: 64

The loss function is:    \item Target update interval: 1,000 steps

\[\end{itemize}

L(\theta) = \mathbb{E}_{(s,a,r,s') \sim \mathcal{D}} \left[ \left( r + \gamma Q_{\theta^-}(s', \arg\max_{a'} Q_\theta(s', a')) - Q_\theta(s,a) \right)^2 \right]

\]\subsubsection*{Preprocessing for Pong}

For the Pong environment:

\subsubsection{Neural Network Architecture}\begin{enumerate}

    \item Convert RGB frames to grayscale.

\textbf{Pong CNN Architecture:}    \item Downsample the image.

\begin{itemize}    \item Subtract consecutive frames to capture motion.

    \item Input: 4 stacked 84×84 grayscale frames\end{enumerate}

    \item Conv1: 32 filters, 8×8 kernel, stride 4, ReLU

    \item Conv2: 64 filters, 4×4 kernel, stride 2, ReLU\subsection{(c) Results and Learning Curves}

    \item Conv3: 64 filters, 3×3 kernel, stride 1, ReLU\begin{figure}[H]

    \item FC1: 512 units, ReLU\centering

    \item Output: 6 Q-values (one per action)\includegraphics[width=0.8\textwidth]{dqn_mountaincar_curve.png}

\end{itemize}\caption{Learning curve for DQN on MountainCar-v0 (mean reward per episode).}

\end{figure}

\textbf{MountainCar MLP Architecture:}

\begin{itemize}\begin{figure}[H]

    \item Input: 2-dimensional state\centering

    \item Hidden1: 128 units, ReLU\includegraphics[width=0.8\textwidth]{dqn_pong_curve.png}

    \item Hidden2: 128 units, ReLU\caption{Learning curve for DQN on Pong-v0.}

    \item Output: 3 Q-values\end{figure}

\end{itemize}

\subsection{(d) Hyperparameter Study}

\subsubsection{Preprocessing for Pong}The learning rate was varied as follows: $\{1 \times 10^{-2}, 1 \times 10^{-3}, 1 \times 10^{-4}, 5 \times 10^{-5}\}$.

\begin{enumerate}

    \item Convert RGB to grayscale\begin{figure}[H]

    \item Crop to remove scoreboard (35:195 rows)\centering

    \item Downsample to 84×84 pixels\includegraphics[width=0.8\textwidth]{lr_comparison.png}

    \item Normalize pixel values to [0, 1]\caption{Effect of learning rate on DQN performance (MountainCar-v0).}

    \item Stack 4 consecutive frames for motion information\end{figure}

\end{enumerate}

\textbf{Observation:} Extremely high or low learning rates led to instability or slow convergence. The best trade-off was found at $\alpha = 1e{-3}$.

\subsection{Hyperparameters}

\subsection{(e) Discussion}

\begin{table}[H]DQN successfully learned to solve the MountainCar problem, with smooth convergence after sufficient exploration. In Pong, preprocessing was crucial for learning meaningful motion-based representations. Training required several million steps for stable performance.

\centering

\caption{DQN Hyperparameters}% ---------------------------------------------------

\begin{tabular}{lcc}\section{Problem 2: Policy Gradient}

\toprule

\textbf{Parameter} & \textbf{Pong} & \textbf{MountainCar} \\\subsection{(a) Environment Setup and Random Agent}

\midrule\begin{lstlisting}[language=Python, caption=Loading CartPole Environment]

Learning Rate & $2.5 \times 10^{-4}$ & $1 \times 10^{-3}$ \\env = gym.make("CartPole-v0")

Discount Factor ($\gamma$) & 0.99 & 0.99 \\print("State space:", env.observation_space)

Replay Buffer Size & 50,000 & 50,000 \\print("Action space:", env.action_space)

Batch Size & 32 & 32 \\\end{lstlisting}

Target Update Frequency & 10,000 steps & 1,000 steps \\

Initial Epsilon ($\epsilon_0$) & 1.0 & 1.0 \\\textbf{Observation:}

Final Epsilon ($\epsilon_{min}$) & 0.02 & 0.01 \\\begin{itemize}

Epsilon Decay & 0.9999 & 0.995 \\    \item Continuous state space of dimension 4.

Optimizer & Adam & Adam \\    \item Two discrete actions: left or right.

\bottomrule\end{itemize}

\end{tabular}

\end{table}\subsection{(b) Implementation Details}

The policy gradient algorithm (REINFORCE) optimizes:

\subsection{Training Results}\[

\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} \left[ \sum_t \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]

\subsubsection{Pong-v5}\]



\textbf{Training Configuration:}Where $G_t$ is either:

\begin{itemize}\begin{itemize}

    \item Total Training Steps: 3,000,000 (3M)    \item \textbf{Total reward}: $G_0 = \sum_{t=0}^T \gamma^t r_{t+1}$

    \item Training divided into 60 batches of 50,000 steps each    \item \textbf{Reward-to-go}: $G_t = \sum_{t'=t}^T \gamma^{t'-t} r_{t'+1}$

    \item Total Episodes: 1,249\end{itemize}

    \item Training Time: $\sim$6-7 hours on RTX 3050

\end{itemize}To reduce variance, we use:

\[

\textbf{Performance Metrics:}A_t = G_t - b(s_t)

\begin{table}[H]\]

\centeringwhere $b(s_t)$ is a baseline (e.g., value function). Advantage normalization ensures $\text{mean}(A_t) = 0$ and $\text{std}(A_t) = 1$.

\caption{Pong DQN Final Performance}

\begin{tabular}{lc}\subsection{(c) Results and Comparisons}

\toprule\begin{figure}[H]

\textbf{Metric} & \textbf{Value} \\\centering

\midrule\includegraphics[width=0.8\textwidth]{pg_cartpole_comparison.png}

Best Mean Reward (100 ep) & +11.75 \\\caption{Policy Gradient on CartPole-v0: with/without Reward-to-Go and Advantage Normalization.}

Last 100 Episodes Mean & +11.19 \\\end{figure}

Last 50 Episodes Mean & +10.76 \\

Win Rate (Last 100 ep) & 97\% \\\begin{figure}[H]

Max Reward Achieved & +20 \\\centering

Final Epsilon & 0.02 \\\includegraphics[width=0.8\textwidth]{pg_lunarlander_comparison.png}

\bottomrule\caption{Policy Gradient on LunarLander-v2: variance reduction effects.}

\end{tabular}\end{figure}

\end{table}

\textbf{Observation:} Both reward-to-go and advantage normalization significantly stabilized training and accelerated convergence.

\textbf{Key Findings:}

\begin{itemize}\subsection{(d) Effect of Batch Size}

    \item Agent achieved 97\% win rate against built-in opponent\begin{figure}[H]

    \item Average score of +11.19 indicates consistent winning performance\centering

    \item In Pong, scores range from -21 to +21\includegraphics[width=0.8\textwidth]{batch_size_effect.png}

    \item Agent successfully learned to track ball and position paddle\caption{Effect of batch size on Policy Gradient performance.}

    \item Training converged after $\sim$2M steps, continued to improve through 3M\end{figure}

\end{itemize}

Larger batch sizes led to smoother gradients and more stable returns but increased computation per iteration.

\begin{figure}[H]

\centering\subsection{(e) Discussion}

\includegraphics[width=0.95\textwidth]{results/pong_training_3M_results.png}Variance reduction techniques substantially improved learning stability, especially in environments with high reward stochasticity such as LunarLander.

\caption{Pong DQN Training Results (3M steps): Learning curves showing episode rewards, reward distribution, loss progression, and performance across 60 training batches}

\end{figure}% ---------------------------------------------------

\section{Conclusions and Insights}

\subsubsection{MountainCar-v0}\begin{itemize}

    \item DQN performs well on discrete environments with visual input, but requires careful tuning and large replay buffers.

\textbf{Performance Metrics:}    \item Policy Gradient methods are more suitable for continuous control, and variance reduction is essential for stability.

\begin{itemize}    \item Reward-to-go and advantage normalization yield more consistent gradient estimates.

    \item Success Rate: 78-88\% (episodes where goal is reached)\end{itemize}

    \item Average Reward: -147 to -154

    \item The agent learned to reach the goal but took many steps\textbf{Future Work:}

    \item Successfully employed momentum-building strategy\begin{itemize}

\end{itemize}    \item Implementing Double DQN or Dueling DQN architectures.

    \item Extending Policy Gradient to Actor-Critic and PPO frameworks.

\begin{figure}[H]\end{itemize}

\centering

\includegraphics[width=0.8\textwidth]{mountaincar_training_results.png}% ---------------------------------------------------

\caption{MountainCar DQN Training: Episode rewards and moving average showing learning progression}\section{References}

\end{figure}\begin{enumerate}

    \item Mnih, V. et al., \textit{Playing Atari with Deep Reinforcement Learning}, NIPS Deep Learning Workshop, 2013.

\subsection{Hyperparameter Study: Learning Rate}    \item Sutton, R. S. and Barto, A. G., \textit{Reinforcement Learning: An Introduction}, 2nd Edition, MIT Press.

    \item OpenAI Gym Documentation: \url{https://www.gymlibrary.dev}

A systematic study was conducted with four different learning rates:\end{enumerate}

\begin{itemize}

    \item $\alpha = 0.0001$\end{document}

    \item $\alpha = 0.001$
    \item $\alpha = 0.01$
    \item $\alpha = 0.1$
\end{itemize}

\begin{table}[H]
\centering
\caption{Learning Rate Study Results (MountainCar)}
\begin{tabular}{lc}
\toprule
\textbf{Learning Rate} & \textbf{Best Mean Reward} \\
\midrule
$1 \times 10^{-4}$ & -199.45 \\
$1 \times 10^{-3}$ & -200.00 \\
$1 \times 10^{-2}$ & -200.00 \\
$1 \times 10^{-1}$ & -200.00 \\
\bottomrule
\end{tabular}
\end{table}

\begin{figure}[H]
\centering
\includegraphics[width=0.8\textwidth]{mountaincar_lr_comparison.png}
\caption{Learning Rate Comparison: Effect on MountainCar DQN performance}
\end{figure}

\textbf{Observations:}
\begin{itemize}
    \item $\alpha = 0.0001$ showed slightly better performance
    \item Higher learning rates ($\geq 0.01$) led to instability
    \item Lower learning rates require more training time
    \item Optimal range: $[10^{-4}, 10^{-3}]$ for this environment
\end{itemize}

% ---------------------------------------------------
\section{Problem 3: Policy Gradient}

\subsection{Environment Analysis}

\subsubsection{CartPole-v1}
\begin{itemize}
    \item \textbf{State Space}: Box(4) - cart position, velocity, pole angle, pole angular velocity
    \item \textbf{Action Space}: Discrete(2) - push left or right
    \item \textbf{Reward}: +1 for each timestep pole remains upright
    \item \textbf{Solved Threshold}: Average reward $\geq$ 475 over 100 episodes
    \item \textbf{Episode Termination}: Pole angle $> \pm 12°$ or cart position $> \pm 2.4$ or 500 steps
\end{itemize}

\subsubsection{LunarLander-v3}
\begin{itemize}
    \item \textbf{State Space}: Box(8) - position, velocity, angle, angular velocity, leg contact
    \item \textbf{Action Space}: Discrete(4) - \{do nothing, fire left, fire main, fire right\}
    \item \textbf{Reward}: Based on distance to landing pad, velocity, crashes, fuel usage
    \item \textbf{Solved Threshold}: Average reward $\geq$ 200 over 100 episodes
    \item \textbf{Reward Range}: [-∞, +200+] (crash: -100, land: +100-140)
\end{itemize}

\subsection{Random Agent Baseline}

\textbf{CartPole-v1 Random Agent:}
\begin{itemize}
    \item Average Reward: $19.4 \pm 10.26$
    \item Max Reward: 42.0
    \item Min Reward: 8.0
\end{itemize}

\subsection{Policy Gradient Algorithm}

\subsubsection{REINFORCE with Variance Reduction}

The policy gradient theorem:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta} \left[ \sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_t | s_t) G_t \right]
\]

\textbf{Return Computation:}
\begin{itemize}
    \item \textbf{Total Trajectory Reward}: $G_0 = \sum_{t=0}^T \gamma^t r_{t+1}$ (same for all timesteps)
    \item \textbf{Reward-to-Go}: $G_t = \sum_{t'=t}^T \gamma^{t'-t} r_{t'+1}$ (reduces variance)
\end{itemize}

\textbf{Baseline Functions:}
\begin{enumerate}
    \item \textbf{No Baseline}: $b = 0$
    \item \textbf{Constant}: $b = \mathbb{E}[G(\tau)] \approx \frac{1}{K} \sum_i G(\tau^i)$
    \item \textbf{Time-dependent}: $b_t = \frac{1}{K} \sum_i G_t(\tau^i)$
    \item \textbf{State-dependent}: $b(s) = V^\pi(s)$ (learned value function)
\end{enumerate}

\textbf{Advantage Estimation:}
\[
A_t = G_t - b(s_t)
\]

\textbf{Advantage Normalization:}
\[
\hat{A}_t = \frac{A_t - \mu(A)}{\sigma(A) + \epsilon}
\]
where $\mu(A)$ is mean, $\sigma(A)$ is standard deviation, and $\epsilon = 10^{-8}$ for numerical stability.

\subsection{Neural Network Architecture}

\textbf{Policy Network (Actor):}
\begin{itemize}
    \item Input: State dimension
    \item Hidden1: 128 units, ReLU
    \item Hidden2: 128 units, ReLU
    \item Output: Action probabilities (Softmax)
\end{itemize}

\textbf{Value Network (Critic - for state baseline):}
\begin{itemize}
    \item Input: State dimension
    \item Hidden1: 128 units, ReLU
    \item Hidden2: 128 units, ReLU
    \item Output: State value $V(s)$
\end{itemize}

\subsection{Hyperparameters}

\begin{table}[H]
\centering
\caption{Policy Gradient Hyperparameters}
\begin{tabular}{lcc}
\toprule
\textbf{Parameter} & \textbf{CartPole} & \textbf{LunarLander} \\
\midrule
Learning Rate (Policy) & 0.001 & 0.001 \\
Learning Rate (Value) & 0.001 & 0.001 \\
Discount Factor ($\gamma$) & 0.99 & 0.99 \\
Iterations & 500 & 500 \\
Batch Size (episodes) & 20 & 20 \\
Optimizer & Adam & Adam \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Training Results}

\subsubsection{CartPole-v1 Results}

\begin{table}[H]
\centering
\caption{CartPole-v1 Policy Gradient Comparison (500 iterations, batch size 20)}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{RTG} & \textbf{Norm} & \textbf{Baseline} & \textbf{Best Reward} \\
\midrule
Baseline & No & No & None & 39.75 \\
RTG Only & Yes & No & None & 24.25 \\
RTG + Norm & Yes & Yes & None & 362.55 \\
RTG + Norm + Constant & Yes & Yes & Constant & \textbf{404.85} \\
RTG + Norm + State & Yes & Yes & State V(s) & 64.40 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item RTG + Normalization + Constant Baseline achieved best performance: 404.85
    \item Close to solving threshold (475) but needed more iterations
    \item Normalization alone (without baseline) can hurt performance
    \item Baseline subtraction is critical for variance reduction
    \item 10.2x improvement from baseline (39.75 → 404.85)
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{results/CartPole-v1_pg_comparison.png}
\caption{CartPole-v1 Policy Gradient: Learning curves comparing different variance reduction techniques}
\end{figure}

\subsubsection{LunarLander-v3 Results}

\begin{table}[H]
\centering
\caption{LunarLander-v3 Policy Gradient Comparison (500 iterations, batch size 20)}
\begin{tabular}{lcccc}
\toprule
\textbf{Configuration} & \textbf{RTG} & \textbf{Norm} & \textbf{Baseline} & \textbf{Best Reward} \\
\midrule
Baseline & No & No & None & -113.09 \\
RTG Only & Yes & No & None & 133.46 \\
RTG + Norm & Yes & Yes & None & 187.90 \\
RTG + Norm + Constant & Yes & Yes & Constant & \textbf{211.65} \\
RTG + Norm + State & Yes & Yes & State V(s) & 191.19 \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Key Findings:}
\begin{itemize}
    \item RTG + Norm + Constant Baseline performed best: 211.65
    \item Successfully solved the environment (threshold: 200)
    \item Dramatic improvement from -113.09 to +211.65 (324.74 point increase)
    \item All variance reduction techniques showed significant gains
    \item Reward-to-go is essential for environments with delayed rewards
\end{itemize}

\begin{figure}[H]
\centering
\includegraphics[width=0.95\textwidth]{results/LunarLander-v3_pg_comparison.png}
\caption{LunarLander-v3 Policy Gradient: Learning curves showing dramatic improvement with variance reduction}
\end{figure}

\subsection{Command-Line Interface}

The implementation includes a flexible command-line interface supporting all configurations:

\begin{lstlisting}[language=bash, caption=Policy Gradient Command-Line Usage]
# With all variance reduction techniques
python policy_gradient.py --env CartPole-v1 --reward_to_go \
    --normalize_advantages --baseline state --iterations 500 \
    --batch_size 20

# Run full comparison automatically
python policy_gradient.py --env CartPole-v1 --iterations 500 \
    --batch_size 20 --compare

# LunarLander training
python policy_gradient.py --env LunarLander-v3 --reward_to_go \
    --normalize_advantages --baseline constant --iterations 500 \
    --batch_size 20 --compare
\end{lstlisting}

% ---------------------------------------------------
\section{Analysis and Discussion}

\subsection{DQN vs Policy Gradient}

\begin{table}[H]
\centering
\caption{Comparison of DQN and Policy Gradient Methods}
\begin{tabular}{lcc}
\toprule
\textbf{Aspect} & \textbf{DQN} & \textbf{Policy Gradient} \\
\midrule
Learning Type & Value-based & Policy-based \\
Action Space & Discrete only & Discrete \& Continuous \\
Sample Efficiency & High (replay buffer) & Lower (on-policy) \\
Stability & Requires target network & Requires variance reduction \\
Convergence & Can oscillate & Smoother with baselines \\
Memory Usage & High (replay buffer) & Low \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Variance Reduction Analysis}

\textbf{Impact of Each Technique:}

\begin{enumerate}
    \item \textbf{Reward-to-Go}:
    \begin{itemize}
        \item Reduces variance by using causal returns
        \item Essential for environments with delayed rewards
        \item CartPole: Minimal improvement alone
        \item LunarLander: 246.55 point improvement (-113.09 → 133.46)
    \end{itemize}
    
    \item \textbf{Advantage Normalization}:
    \begin{itemize}
        \item Standardizes advantage estimates
        \item Can hurt performance without good baseline
        \item Works best when combined with baseline subtraction
        \item Ensures stable gradient magnitudes
    \end{itemize}
    
    \item \textbf{Baseline Subtraction}:
    \begin{itemize}
        \item Most critical variance reduction technique
        \item Constant baseline: Simple, effective
        \item State-dependent baseline: Best theoretical properties
        \item CartPole: 365.10 point improvement (39.75 → 404.85)
    \end{itemize}
\end{enumerate}

\subsection{Computational Requirements}

\begin{table}[H]
\centering
\caption{Training Time and Computational Requirements}
\begin{tabular}{lccc}
\toprule
\textbf{Task} & \textbf{Steps/Iterations} & \textbf{Time} & \textbf{Episodes} \\
\midrule
Pong DQN (3M) & 3,000,000 & $\sim$6-7 hours & 1,249 \\
MountainCar DQN & 100,000 & $\sim$30 min & $\sim$500 \\
CartPole PG (5 configs) & 500 × 5 & $\sim$20 min & $\sim$50,000 \\
LunarLander PG (5 configs) & 500 × 5 & $\sim$2 hours & $\sim$50,000 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Challenges and Solutions}

\textbf{Challenges Encountered:}
\begin{enumerate}
    \item \textbf{Pong Training Stability}:
    \begin{itemize}
        \item Problem: GPU memory overflow with large replay buffer
        \item Solution: Batch training (50K step chunks), memory cleanup between batches
    \end{itemize}
    
    \item \textbf{MountainCar Sparse Rewards}:
    \begin{itemize}
        \item Problem: Agent struggles to discover goal initially
        \item Solution: Long exploration period, careful epsilon decay
    \end{itemize}
    
    \item \textbf{CartPole High Variance}:
    \begin{itemize}
        \item Problem: Policy gradient unstable without variance reduction
        \item Solution: Implemented multiple baseline types and normalization
    \end{itemize}
    
    \item \textbf{LunarLander Box2D Dependency}:
    \begin{itemize}
        \item Problem: Box2D physics library installation issues on Windows
        \item Solution: Used `pip install Box2D --no-build-isolation`
    \end{itemize}
\end{enumerate}

% ---------------------------------------------------
\section{Conclusions}

\subsection{Key Findings}

\begin{enumerate}
    \item \textbf{DQN Performance}:
    \begin{itemize}
        \item Successfully mastered Pong with 97\% win rate after 3M steps
        \item Achieved 78-88\% success rate on MountainCar
        \item CNN architecture essential for visual environments
        \item Experience replay and target networks crucial for stability
    \end{itemize}
    
    \item \textbf{Policy Gradient Performance}:
    \begin{itemize}
        \item Variance reduction techniques critical for success
        \item RTG + Norm + Baseline achieved 10x improvement on CartPole
        \item Successfully solved LunarLander (211.65 vs 200 threshold)
        \item Baseline subtraction most impactful single technique
    \end{itemize}
    
    \item \textbf{Hyperparameter Sensitivity}:
    \begin{itemize}
        \item Learning rate optimal range: $[10^{-4}, 10^{-3}]$
        \item Batch size affects stability vs computation trade-off
        \item Epsilon decay rate influences exploration-exploitation balance
    \end{itemize}
\end{enumerate}

\subsection{Comparative Analysis}

\textbf{When to Use DQN:}
\begin{itemize}
    \item Discrete action spaces
    \item Visual/high-dimensional observations
    \item When sample efficiency is important
    \item Environments where off-policy learning is beneficial
\end{itemize}

\textbf{When to Use Policy Gradient:}
\begin{itemize}
    \item Continuous action spaces
    \item When direct policy optimization is needed
    \item Simpler implementation for discrete spaces
    \item When on-policy learning is acceptable
\end{itemize}

\subsection{Future Improvements}

\begin{enumerate}
    \item \textbf{DQN Extensions}:
    \begin{itemize}
        \item Implement Dueling DQN architecture
        \item Try Prioritized Experience Replay
        \item Explore Rainbow DQN combining multiple improvements
    \end{itemize}
    
    \item \textbf{Policy Gradient Extensions}:
    \begin{itemize}
        \item Implement Actor-Critic methods (A2C/A3C)
        \item Explore Proximal Policy Optimization (PPO)
        \item Test Generalized Advantage Estimation (GAE)
    \end{itemize}
    
    \item \textbf{Training Optimization}:
    \begin{itemize}
        \item Implement distributed training
        \item Automatic hyperparameter tuning
        \item Early stopping based on performance metrics
    \end{itemize}
\end{enumerate}

% ---------------------------------------------------
\section{Code Structure and Reproducibility}

\subsection{File Organization}
\begin{lstlisting}
assignment/
├── dqn_pong.py                    # Pong DQN implementation
├── dqn_pong_continue.py           # Continue training from checkpoint
├── dqn_quickstart.py              # MountainCar DQN
├── mountaincar_hyperparameter_study.py  # LR study
├── evaluate_pong_3M.py            # Pong evaluation with rendering
├── evaluate_mountaincar.py        # MountainCar evaluation
├── policy_gradient.py             # PG with variance reduction
├── evaluate_best_pg.py            # PG model evaluation
└── results/
    ├── pong_dqn_3M_final.pth      # Final Pong model
    ├── mountaincar_dqn_final.pth  # MountainCar model
    ├── CartPole-v1_*_best.pth     # CartPole PG models (5)
    ├── LunarLander-v3_*_best.pth  # LunarLander PG models (5)
    └── *.png                      # Training plots
\end{lstlisting}

\subsection{Reproducibility}

All experiments can be reproduced using the provided code and hyperparameters. Random seeds were set (SEED=42) for consistency. The command-line interface allows exact replication of any configuration.

% ---------------------------------------------------
\section{References}

\begin{enumerate}
    \item Mnih, V., et al. (2015). \textit{Human-level control through deep reinforcement learning}. Nature, 518(7540), 529-533.
    
    \item Van Hasselt, H., Guez, A., \& Silver, D. (2016). \textit{Deep reinforcement learning with double Q-learning}. In AAAI Conference on Artificial Intelligence.
    
    \item Williams, R. J. (1992). \textit{Simple statistical gradient-following algorithms for connectionist reinforcement learning}. Machine Learning, 8(3-4), 229-256.
    
    \item Sutton, R. S., \& Barto, A. G. (2018). \textit{Reinforcement Learning: An Introduction} (2nd ed.). MIT Press.
    
    \item Schulman, J., et al. (2015). \textit{High-dimensional continuous control using generalized advantage estimation}. arXiv preprint arXiv:1506.02438.
    
    \item Gymnasium Documentation. \url{https://gymnasium.farama.org/}
    
    \item PyTorch Documentation. \url{https://pytorch.org/docs/}
\end{enumerate}

\end{document}

